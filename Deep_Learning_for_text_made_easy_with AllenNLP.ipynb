{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning for text made easy with AllenNLP",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "9qff7OZUnMNE",
        "9snXs4hdnJfa",
        "7kamp0DTR7KI",
        "sl05yTZMULwg",
        "X5JhIL1GUPhp"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bond-SYSU/nlp_multi_task_learning_pytorch/blob/master/Deep_Learning_for_text_made_easy_with%C2%A0AllenNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "9qff7OZUnMNE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Install [AllenNLP](http://allennlp.org/)"
      ]
    },
    {
      "metadata": {
        "id": "yUpruEZ9kumk",
        "colab_type": "code",
        "outputId": "312d7420-54b5-40be-9f25-24931d39b75c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3146
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install allennlp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp\n",
            "  Downloading allennlp-0.4.0-py3-none-any.whl (488kB)\n",
            "\u001b[K    100% |████████████████████████████████| 491kB 2.2MB/s \n",
            "\u001b[?25hCollecting argparse (from allennlp)\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp)\n",
            "Collecting gevent==1.2.2 (from allennlp)\n",
            "  Downloading gevent-1.2.2-cp36-cp36m-manylinux1_x86_64.whl (1.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.7MB 852kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp)\n",
            "Collecting psycopg2 (from allennlp)\n",
            "  Downloading psycopg2-2.7.4-cp36-cp36m-manylinux1_x86_64.whl (2.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.7MB 551kB/s \n",
            "\u001b[?25hCollecting pytz==2017.3 (from allennlp)\n",
            "  Downloading pytz-2017.3-py2.py3-none-any.whl (511kB)\n",
            "\u001b[K    100% |████████████████████████████████| 512kB 3.1MB/s \n",
            "\u001b[?25hCollecting overrides (from allennlp)\n",
            "  Downloading overrides-1.9.tar.gz\n",
            "Collecting pyhocon==0.3.35 (from allennlp)\n",
            "  Downloading pyhocon-0.3.35.tar.gz (94kB)\n",
            "\u001b[K    100% |████████████████████████████████| 102kB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp)\n",
            "Collecting tensorboardX==1.0 (from allennlp)\n",
            "  Downloading tensorboardX-1.0-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 9.7MB/s \n",
            "\u001b[?25hCollecting cffi==1.11.2 (from allennlp)\n",
            "  Downloading cffi-1.11.2-cp36-cp36m-manylinux1_x86_64.whl (419kB)\n",
            "\u001b[K    100% |████████████████████████████████| 430kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp)\n",
            "Collecting flask==0.12.1 (from allennlp)\n",
            "  Downloading Flask-0.12.1-py2.py3-none-any.whl (82kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 8.2MB/s \n",
            "\u001b[?25hCollecting tqdm>=4.19 (from allennlp)\n",
            "  Downloading tqdm-4.19.7-py2.py3-none-any.whl (52kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 11.7MB/s \n",
            "\u001b[?25hCollecting typing (from allennlp)\n",
            "  Downloading typing-3.6.4-py3-none-any.whl\n",
            "Collecting spacy<2.1,>=2.0 (from allennlp)\n",
            "  Downloading spacy-2.0.9.tar.gz (17.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 17.5MB 82kB/s \n",
            "\u001b[?25hCollecting flask-cors==3.0.3 (from allennlp)\n",
            "  Downloading Flask_Cors-3.0.3-py2.py3-none-any.whl\n",
            "Collecting awscli>=1.11.91 (from allennlp)\n",
            "  Downloading awscli-1.14.55-py2.py3-none-any.whl (1.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.2MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp)\n",
            "Collecting editdistance (from allennlp)\n",
            "  Downloading editdistance-0.4-cp36-cp36m-manylinux1_x86_64.whl (174kB)\n",
            "\u001b[K    100% |████████████████████████████████| 184kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp)\n",
            "Collecting greenlet>=0.4.10 (from gevent==1.2.2->allennlp)\n",
            "  Downloading greenlet-0.4.13-cp36-cp36m-manylinux1_x86_64.whl (43kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 11.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp)\n",
            "Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from pyhocon==0.3.35->allennlp)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.0->allennlp)\n",
            "Requirement already satisfied: protobuf>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.0->allennlp)\n",
            "Collecting pycparser (from cffi==1.11.2->allennlp)\n",
            "  Downloading pycparser-2.18.tar.gz (245kB)\n",
            "\u001b[K    99% |████████████████████████████████| 245kB 49.7MB/s eta 0:00:01"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[K    100% |████████████████████████████████| 256kB 4.4MB/s \n",
            "\u001b[?25hCollecting click>=2.0 (from flask==0.12.1->allennlp)\n",
            "  Downloading click-6.7-py2.py3-none-any.whl (71kB)\n",
            "\u001b[K    100% |████████████████████████████████| 71kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.6/dist-packages (from flask==0.12.1->allennlp)\n",
            "Requirement already satisfied: Jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from flask==0.12.1->allennlp)\n",
            "Collecting itsdangerous>=0.21 (from flask==0.12.1->allennlp)\n",
            "  Downloading itsdangerous-0.24.tar.gz (46kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 9.7MB/s \n",
            "\u001b[?25hCollecting cymem<1.32,>=1.30 (from spacy<2.1,>=2.0->allennlp)\n",
            "  Downloading cymem-1.31.2.tar.gz\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp)\n",
            "Collecting ftfy<5.0.0,>=4.4.2 (from spacy<2.1,>=2.0->allennlp)\n",
            "  Downloading ftfy-4.4.3.tar.gz (50kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 10.0MB/s \n",
            "\u001b[?25hCollecting html5lib==1.0b8 (from spacy<2.1,>=2.0->allennlp)\n",
            "  Downloading html5lib-1.0b8.tar.gz (889kB)\n",
            "\u001b[K    100% |████████████████████████████████| 890kB 1.5MB/s \n",
            "\u001b[?25hCollecting msgpack-numpy==0.4.1 (from spacy<2.1,>=2.0->allennlp)\n",
            "  Downloading msgpack_numpy-0.4.1-py2.py3-none-any.whl\n",
            "Collecting msgpack-python==0.5.4 (from spacy<2.1,>=2.0->allennlp)\n",
            "  Downloading msgpack-python-0.5.4.tar.gz\n",
            "Collecting murmurhash<0.29,>=0.28 (from spacy<2.1,>=2.0->allennlp)\n",
            "  Downloading murmurhash-0.28.0.tar.gz\n",
            "Collecting pathlib (from spacy<2.1,>=2.0->allennlp)\n",
            "  Downloading pathlib-1.0.1.tar.gz (49kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 9.7MB/s \n",
            "\u001b[?25hCollecting plac<1.0.0,>=0.9.6 (from spacy<2.1,>=2.0->allennlp)\n",
            "  Downloading plac-0.9.6-py2.py3-none-any.whl\n",
            "Collecting preshed<2.0.0,>=1.0.0 (from spacy<2.1,>=2.0->allennlp)\n",
            "  Downloading preshed-1.0.0.tar.gz (89kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 10.9MB/s \n",
            "\u001b[?25hCollecting regex==2017.4.5 (from spacy<2.1,>=2.0->allennlp)\n",
            "  Downloading regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K    100% |████████████████████████████████| 604kB 2.2MB/s \n",
            "\u001b[?25hCollecting thinc<6.11.0,>=6.10.1 (from spacy<2.1,>=2.0->allennlp)\n",
            "  Downloading thinc-6.10.2.tar.gz (1.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.2MB 1.1MB/s \n",
            "\u001b[?25hCollecting ujson>=1.35 (from spacy<2.1,>=2.0->allennlp)\n",
            "  Downloading ujson-1.35.tar.gz (192kB)\n",
            "\u001b[K    100% |████████████████████████████████| 194kB 4.7MB/s \n",
            "\u001b[?25hCollecting botocore==1.9.8 (from awscli>=1.11.91->allennlp)\n",
            "  Downloading botocore-1.9.8-py2.py3-none-any.whl (4.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 4.1MB 342kB/s \n",
            "\u001b[?25hRequirement already satisfied: rsa<=3.5.0,>=3.1.2 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp)\n",
            "Collecting docutils>=0.10 (from awscli>=1.11.91->allennlp)\n",
            "  Downloading docutils-0.14-py3-none-any.whl (543kB)\n",
            "\u001b[K    100% |████████████████████████████████| 552kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML<=3.12,>=3.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp)\n",
            "Collecting colorama<=0.3.7,>=0.2.5 (from awscli>=1.11.91->allennlp)\n",
            "  Downloading colorama-0.3.7-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.2.0,>=0.1.12 (from awscli>=1.11.91->allennlp)\n",
            "  Downloading s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 11.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from protobuf>=0.3.2->tensorboardX==1.0->allennlp)\n",
            "Requirement already satisfied: MarkupSafe in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.4->flask==0.12.1->allennlp)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy<2.1,>=2.0->allennlp)\n",
            "Collecting cytoolz<0.9,>=0.8 (from thinc<6.11.0,>=6.10.1->spacy<2.1,>=2.0->allennlp)\n",
            "  Downloading cytoolz-0.8.2.tar.gz (386kB)\n",
            "\u001b[K    100% |████████████████████████████████| 389kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.1->spacy<2.1,>=2.0->allennlp)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting wrapt (from thinc<6.11.0,>=6.10.1->spacy<2.1,>=2.0->allennlp)\n",
            "  Downloading wrapt-1.10.11.tar.gz\n",
            "Collecting jmespath<1.0.0,>=0.7.1 (from botocore==1.9.8->awscli>=1.11.91->allennlp)\n",
            "  Downloading jmespath-0.9.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil<2.7.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore==1.9.8->awscli>=1.11.91->allennlp)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<=3.5.0,>=3.1.2->awscli>=1.11.91->allennlp)\n",
            "Collecting toolz>=0.8.0 (from cytoolz<0.9,>=0.8->thinc<6.11.0,>=6.10.1->spacy<2.1,>=2.0->allennlp)\n",
            "  Downloading toolz-0.9.0.tar.gz (45kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 12.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: overrides, pyhocon, spacy, pycparser, itsdangerous, cymem, ftfy, html5lib, msgpack-python, murmurhash, pathlib, preshed, regex, thinc, ujson, cytoolz, wrapt, toolz\n",
            "  Running setup.py bdist_wheel for overrides ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/6e/88/72/8d3d52a86af3b3f6e1f5e55f58997f6aea93ca798bf7907b99\n",
            "  Running setup.py bdist_wheel for pyhocon ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/18/b1/ce/73dd2a26dc61bcf60716b01ebd816d2e9081a686e1dd91f6a4\n",
            "  Running setup.py bdist_wheel for spacy ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/0a/7a/92/23f94d6a3e10192bdffa2d20836e28271801b459915c568049\n",
            "  Running setup.py bdist_wheel for pycparser ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/95/14/9a/5e7b9024459d2a6600aaa64e0ba485325aff7a9ac7489db1b6\n",
            "  Running setup.py bdist_wheel for itsdangerous ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/fc/a8/66/24d655233c757e178d45dea2de22a04c6d92766abfb741129a\n",
            "  Running setup.py bdist_wheel for cymem ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/4b/2a/0e/dce3ff7a6f0f916906ef978afe512a7b5aef8abfd9ba988acf\n",
            "  Running setup.py bdist_wheel for ftfy ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/ae/d7/4c/339066248431397227741c7fdc80ad85826188ee9b0c24b4c7\n",
            "  Running setup.py bdist_wheel for html5lib ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/d4/d1/0b/a6b6f9f204af55c9bb8c97eae2a78b690b7150a7b850bb9403\n",
            "  Running setup.py bdist_wheel for msgpack-python ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/49/fc/5d/07243edfc74536e0776f3f8901418e7596de0e3b887dd17d86\n",
            "  Running setup.py bdist_wheel for murmurhash ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/67/30/e2/ef52a408eda4b23581669abfd1c1516a931d9e53f2e7616cb9\n",
            "  Running setup.py bdist_wheel for pathlib ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/2a/23/a5/d8803db5d631e9f391fe6defe982a238bf5483062eeb34e841\n",
            "  Running setup.py bdist_wheel for preshed ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/97/64/22/20fabf1f51039b799e64e46d0381b023cfdbe159c349d7c135\n",
            "  Running setup.py bdist_wheel for regex ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/28/20/68/c71f468f76d9bd81730a7633ace0ad30507cf99166314109a1\n",
            "  Running setup.py bdist_wheel for thinc ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/f8/fc/92/3bb08540cc5ac05df781005e686273adcd97af91ca2c032154\n",
            "  Running setup.py bdist_wheel for ujson ... \u001b[?25l-"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\b \b\\\b \b|\b \b/\b \b-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/9e/9b/d0/df92653bb5b2664c15d8ee5b99e3f2eb08a034444db8922b2f\n",
            "  Running setup.py bdist_wheel for cytoolz ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/52/92/ed/661ecb7a67b42b21fc3dea140abb9ae9b8e94e72f0b3aff6c1\n",
            "  Running setup.py bdist_wheel for wrapt ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/56/e1/0f/f7ccf1ed8ceaabccc2a93ce0481f73e589814cbbc439291345\n",
            "  Running setup.py bdist_wheel for toolz ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/57/51/8a/433a9c0a2c65fc1b2a795ae036b932f3339a02e9ae88367659\n",
            "Successfully built overrides pyhocon spacy pycparser itsdangerous cymem ftfy html5lib msgpack-python murmurhash pathlib preshed regex thinc ujson cytoolz wrapt toolz\n",
            "Installing collected packages: argparse, greenlet, gevent, psycopg2, pytz, overrides, pyhocon, tensorboardX, pycparser, cffi, click, itsdangerous, flask, tqdm, typing, cymem, html5lib, ftfy, msgpack-python, msgpack-numpy, murmurhash, pathlib, plac, preshed, regex, toolz, cytoolz, wrapt, thinc, ujson, spacy, flask-cors, jmespath, docutils, botocore, colorama, s3transfer, awscli, editdistance, allennlp\n",
            "  Found existing installation: pytz 2016.7\n",
            "    Uninstalling pytz-2016.7:\n",
            "      Successfully uninstalled pytz-2016.7\n",
            "  Found existing installation: html5lib 0.9999999\n",
            "    Uninstalling html5lib-0.9999999:\n",
            "      Successfully uninstalled html5lib-0.9999999\n",
            "Successfully installed allennlp-0.4.0 argparse-1.4.0 awscli-1.14.55 botocore-1.9.8 cffi-1.11.2 click-6.7 colorama-0.3.7 cymem-1.31.2 cytoolz-0.8.2 docutils-0.14 editdistance-0.4 flask-0.12.1 flask-cors-3.0.3 ftfy-4.4.3 gevent-1.2.2 greenlet-0.4.13 html5lib-1.0b8 itsdangerous-0.24 jmespath-0.9.3 msgpack-numpy-0.4.1 msgpack-python-0.5.4 murmurhash-0.28.0 overrides-1.9 pathlib-1.0.1 plac-0.9.6 preshed-1.0.0 psycopg2-2.7.4 pycparser-2.18 pyhocon-0.3.35 pytz-2017.3 regex-2017.4.5 s3transfer-0.1.13 spacy-2.0.9 tensorboardX-1.0 thinc-6.10.2 toolz-0.9.0 tqdm-4.19.7 typing-3.6.4 ujson-1.35 wrapt-1.10.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9snXs4hdnJfa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Install [pytorch](http://pytorch.org/)"
      ]
    },
    {
      "metadata": {
        "id": "9kcAHzR4nB_6",
        "colab_type": "code",
        "outputId": "a55af6a5-e3d7-4976-bd8e-f3af0006ddf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "#!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
        "!pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
        "!pip install torchvision"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.3.0.post4 from http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl\n",
            "  Downloading http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl (592.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 592.3MB 60.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch==0.3.0.post4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==0.3.0.post4)\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-0.3.0.post4\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.2.0-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 2.9MB/s \n",
            "\u001b[?25hCollecting pillow>=4.1.1 (from torchvision)\n",
            "  Downloading Pillow-5.0.0-cp36-cp36m-manylinux1_x86_64.whl (5.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.9MB 231kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch->torchvision)\n",
            "Installing collected packages: pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.0.0 torchvision-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X1xICldXSDyJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Install other dependencies"
      ]
    },
    {
      "metadata": {
        "id": "NIVRns2WtpHT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install overrides\n",
        "!pip install tqdm\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7kamp0DTR7KI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# dataset_reader (fetch_newsgroups.py)"
      ]
    },
    {
      "metadata": {
        "id": "qm81xmtKnIVs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "2Zdwa_x2j-5G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from typing import Dict\n",
        "import json\n",
        "import logging\n",
        "\n",
        "from overrides import overrides\n",
        "\n",
        "import tqdm\n",
        "\n",
        "from allennlp.common import Params\n",
        "from allennlp.common.file_utils import cached_path\n",
        "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
        "from allennlp.data.fields import LabelField, TextField\n",
        "from allennlp.data.instance import Instance\n",
        "from allennlp.data.tokenizers import Tokenizer, WordTokenizer\n",
        "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
        "\n",
        "\n",
        "@DatasetReader.register(\"20newsgroups\")\n",
        "class NewsgroupsDatasetReader(DatasetReader):\n",
        "    \"\"\"\n",
        "    Reads a JSON-lines file containing papers from the Semantic Scholar database, and creates a\n",
        "    dataset suitable for document classification using these papers.\n",
        "\n",
        "    Expected format for each input line: {\"paperAbstract\": \"text\", \"title\": \"text\", \"venue\": \"text\"}\n",
        "\n",
        "    The JSON could have other fields, too, but they are ignored.\n",
        "\n",
        "    The output of ``read`` is a list of ``Instance`` s with the fields:\n",
        "        title: ``TextField``\n",
        "        abstract: ``TextField``\n",
        "        label: ``LabelField``\n",
        "\n",
        "    where the ``label`` is derived from the venue of the paper.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    tokenizer : ``Tokenizer``, optional\n",
        "        Tokenizer to use to split the title and abstrct into words or other kinds of tokens.\n",
        "        Defaults to ``WordTokenizer()``.\n",
        "    token_indexers : ``Dict[str, TokenIndexer]``, optional\n",
        "        Indexers used to define input token representations. Defaults to ``{\"tokens\":\n",
        "        SingleIdTokenIndexer()}``.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 tokenizer: Tokenizer = None,\n",
        "                 token_indexers: Dict[str, TokenIndexer] = None) -> None:\n",
        "        self._tokenizer = tokenizer or WordTokenizer()\n",
        "        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
        "\n",
        "    @overrides\n",
        "    def _read(self, file_path):\n",
        "        instances = []\n",
        "        if file_path == \"train\":\n",
        "            logger.info(\"Reading instances from: %s\", file_path)\n",
        "            categories = [\"comp.graphics\",\"sci.space\",\"rec.sport.baseball\"]\n",
        "            newsgroups_data = fetch_20newsgroups(subset='train',categories=categories)\n",
        "            \n",
        "        elif file_path == \"test\":\n",
        "            logger.info(\"Reading instances from: %s\", file_path)\n",
        "            categories = [\"comp.graphics\",\"sci.space\",\"rec.sport.baseball\"]\n",
        "            newsgroups_data = fetch_20newsgroups(subset='test',categories=categories)\n",
        "            \n",
        "        else:\n",
        "            raise ConfigurationError(\"Path string not specified in read method\")\n",
        "            \n",
        "        for i,text in enumerate(newsgroups_data.data):\n",
        "            if file_path == \"validate\":\n",
        "                if i == 400:\n",
        "                    break\n",
        "            text = newsgroups_data.data[i]\n",
        "            target = newsgroups_data.target[i]\n",
        "            yield self.text_to_instance(text, target)\n",
        "\n",
        "    @overrides\n",
        "    def text_to_instance(self, text: str, target: str = None) -> Instance:  # type: ignore\n",
        "        # pylint: disable=arguments-differ\n",
        "        tokenized_text = self._tokenizer.tokenize(text)\n",
        "        text_field = TextField(tokenized_text, self._token_indexers)\n",
        "        fields = {'text': text_field}\n",
        "        if target is not None:\n",
        "            fields['label'] = LabelField(int(target),skip_indexing=True)\n",
        "        return Instance(fields)\n",
        "\n",
        "    @classmethod\n",
        "    def from_params(cls, params: Params) -> 'NewsgroupsDatasetReader':\n",
        "        tokenizer = Tokenizer.from_params(params.pop('tokenizer', {}))\n",
        "        token_indexers = TokenIndexer.dict_from_params(params.pop('token_indexers', {}))\n",
        "        params.assert_empty(cls.__name__)\n",
        "        return cls(tokenizer=tokenizer, token_indexers=token_indexers)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sl05yTZMULwg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# model (newsgroups_classifier.py)"
      ]
    },
    {
      "metadata": {
        "id": "cuYqgXr6v6q8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from typing import Dict, Optional\n",
        "\n",
        "import numpy\n",
        "from overrides import overrides\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from allennlp.common import Params\n",
        "from allennlp.common.checks import ConfigurationError\n",
        "from allennlp.data import Vocabulary\n",
        "from allennlp.modules import FeedForward, Seq2VecEncoder, TextFieldEmbedder\n",
        "from allennlp.models.model import Model\n",
        "from allennlp.nn import InitializerApplicator, RegularizerApplicator\n",
        "from allennlp.nn import util\n",
        "from allennlp.training.metrics import CategoricalAccuracy\n",
        "\n",
        "\n",
        "@Model.register(\"20newsgroups_classifier\")\n",
        "class Fetch20NewsgroupsClassifier(Model):\n",
        "    \"\"\"\n",
        "    This ``Model`` performs text classification for a newsgroup text.  We assume we're given a\n",
        "    text and we predict some output label.\n",
        "    The basic model structure: we'll embed the text and encode it with\n",
        "    a Seq2VecEncoder, getting a single vector representing the content.  We'll then\n",
        "    the result through a feedforward network, the output of\n",
        "    which we'll use as our scores for each label.\n",
        "    Parameters\n",
        "    ----------\n",
        "    vocab : ``Vocabulary``, required\n",
        "        A Vocabulary, required in order to compute sizes for input/output projections.\n",
        "    model_text_field_embedder : ``TextFieldEmbedder``, required\n",
        "        Used to embed the ``tokens`` ``TextField`` we get as input to the model.\n",
        "    internal_text_encoder : ``Seq2VecEncoder``\n",
        "        The encoder that we will use to convert the input text to a vector.\n",
        "    classifier_feedforward : ``FeedForward``\n",
        "    initializer : ``InitializerApplicator``, optional (default=``InitializerApplicator()``)\n",
        "        Used to initialize the model parameters.\n",
        "    regularizer : ``RegularizerApplicator``, optional (default=``None``)\n",
        "        If provided, will be used to calculate the regularization penalty during training.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab: Vocabulary,\n",
        "                 model_text_field_embedder: TextFieldEmbedder,\n",
        "                 internal_text_encoder: Seq2VecEncoder,\n",
        "                 classifier_feedforward: FeedForward,\n",
        "                 initializer: InitializerApplicator = InitializerApplicator(),\n",
        "                 regularizer: Optional[RegularizerApplicator] = None) -> None:\n",
        "        super(Fetch20NewsgroupsClassifier, self).__init__(vocab, regularizer)\n",
        "\n",
        "        self.model_text_field_embedder = model_text_field_embedder\n",
        "        self.num_classes = self.vocab.get_vocab_size(\"labels\")\n",
        "        self.internal_text_encoder = internal_text_encoder\n",
        "        self.classifier_feedforward = classifier_feedforward\n",
        "\n",
        "        if model_text_field_embedder.get_output_dim() != internal_text_encoder.get_input_dim():\n",
        "            raise ConfigurationError(\"The output dimension of the model_text_field_embedder must match the \"\n",
        "                                     \"input dimension of the title_encoder. Found {} and {}, \"\n",
        "                                     \"respectively.\".format(model_text_field_embedder.get_output_dim(),\n",
        "                                                            internal_text_encoder.get_input_dim()))\n",
        "        self.metrics = {\n",
        "                \"accuracy\": CategoricalAccuracy(),\n",
        "                \"accuracy3\": CategoricalAccuracy(top_k=3)\n",
        "        }\n",
        "        self.loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        initializer(self)\n",
        "\n",
        "    @overrides\n",
        "    def forward(self,  # type: ignore\n",
        "                text: Dict[str, torch.LongTensor],\n",
        "                label: torch.LongTensor = None) -> Dict[str, torch.Tensor]:\n",
        "        # pylint: disable=arguments-differ\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_text : Dict[str, Variable], required\n",
        "            The output of ``TextField.as_array()``.\n",
        "        label : Variable, optional (default = None)\n",
        "            A variable representing the label for each instance in the batch.\n",
        "        Returns\n",
        "        -------\n",
        "        An output dictionary consisting of:\n",
        "        class_probabilities : torch.FloatTensor\n",
        "            A tensor of shape ``(batch_size, num_classes)`` representing a distribution over the\n",
        "            label classes for each instance.\n",
        "        loss : torch.FloatTensor, optional\n",
        "            A scalar loss to be optimised.\n",
        "        \"\"\"\n",
        "        embedded_text = self.model_text_field_embedder(text)\n",
        "        text_mask = util.get_text_field_mask(text)\n",
        "        encoded_text = self.internal_text_encoder(embedded_text, text_mask)\n",
        "\n",
        "        logits = self.classifier_feedforward(encoded_text)\n",
        "        output_dict = {'logits': logits}\n",
        "        if label is not None:\n",
        "            loss = self.loss(logits, label.squeeze(-1))\n",
        "            for metric in self.metrics.values():\n",
        "                metric(logits, label.squeeze(-1))\n",
        "            output_dict[\"loss\"] = loss\n",
        "\n",
        "        return output_dict\n",
        "\n",
        "    @overrides\n",
        "    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Does a simple argmax over the class probabilities, converts indices to string labels, and\n",
        "        adds a ``\"label\"`` key to the dictionary with the result.\n",
        "        \"\"\"\n",
        "        class_probabilities = F.softmax(output_dict['logits'])\n",
        "        output_dict['class_probabilities'] = class_probabilities\n",
        "\n",
        "        predictions = class_probabilities.cpu().data.numpy()\n",
        "        argmax_indices = numpy.argmax(predictions, axis=-1)\n",
        "        labels = [self.vocab.get_token_from_index(x, namespace=\"labels\")\n",
        "                  for x in argmax_indices]\n",
        "        output_dict['label'] = labels\n",
        "        return output_dict\n",
        "\n",
        "    @overrides\n",
        "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
        "        return {metric_name: metric.get_metric(reset) for metric_name, metric in self.metrics.items()}\n",
        "\n",
        "    @classmethod\n",
        "    def from_params(cls, vocab: Vocabulary, params: Params) -> 'Fetch20NewsgroupsClassifier':\n",
        "        embedder_params = params.pop(\"model_text_field_embedder\")\n",
        "        model_text_field_embedder = TextFieldEmbedder.from_params(vocab, embedder_params)\n",
        "        internal_text_encoder = Seq2VecEncoder.from_params(params.pop(\"internal_text_encoder\"))\n",
        "        classifier_feedforward = FeedForward.from_params(params.pop(\"classifier_feedforward\"))\n",
        "\n",
        "        initializer = InitializerApplicator.from_params(params.pop('initializer', []))\n",
        "        regularizer = RegularizerApplicator.from_params(params.pop('regularizer', []))\n",
        "\n",
        "        return cls(vocab=vocab,\n",
        "                   model_text_field_embedder=model_text_field_embedder,\n",
        "                   internal_text_encoder=internal_text_encoder,\n",
        "                   classifier_feedforward=classifier_feedforward,\n",
        "                   initializer=initializer,\n",
        "                   regularizer=regularizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X5JhIL1GUPhp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Wrapper to tun model"
      ]
    },
    {
      "metadata": {
        "id": "Lq3vUflPwENo",
        "colab_type": "code",
        "outputId": "b204f01d-e35c-4ba8-9861-17d33ef93156",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "from typing import Dict, Iterable\n",
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "from copy import deepcopy\n",
        "\n",
        "from allennlp.commands.evaluate import evaluate\n",
        "from allennlp.commands.subcommand import Subcommand\n",
        "from allennlp.common.checks import ConfigurationError\n",
        "from allennlp.common import Params, TeeLogger, Tqdm\n",
        "from allennlp.common.util import prepare_environment\n",
        "from allennlp.data import Vocabulary\n",
        "from allennlp.data.instance import Instance\n",
        "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
        "from allennlp.data.iterators.data_iterator import DataIterator\n",
        "from allennlp.models.archival import archive_model, CONFIG_NAME\n",
        "from allennlp.models.model import Model\n",
        "from allennlp.training.trainer import Trainer\n",
        "\n",
        "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
        "\n",
        "\n",
        "class Train(Subcommand):\n",
        "    def add_subparser(self, name: str, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:\n",
        "        # pylint: disable=protected-access\n",
        "        description = '''Train the specified model on the specified dataset.'''\n",
        "        subparser = parser.add_parser(name, description=description, help='Train a model')\n",
        "\n",
        "        subparser.add_argument('param_path',\n",
        "                               type=str,\n",
        "                               help='path to parameter file describing the model to be trained')\n",
        "\n",
        "        subparser.add_argument('-s', '--serialization-dir',\n",
        "                               required=True,\n",
        "                               type=str,\n",
        "                               help='directory in which to save the model and its logs')\n",
        "\n",
        "        subparser.add_argument('-r', '--recover',\n",
        "                               action='store_true',\n",
        "                               default=False,\n",
        "                               help='recover training from the state in serialization_dir')\n",
        "\n",
        "        subparser.add_argument('-o', '--overrides',\n",
        "                               type=str,\n",
        "                               default=\"\",\n",
        "                               help='a HOCON structure used to override the experiment configuration')\n",
        "\n",
        "        subparser.add_argument('--file-friendly-logging',\n",
        "                               action='store_true',\n",
        "                               default=False,\n",
        "                               help='outputs tqdm status on separate lines and slows tqdm refresh rate')\n",
        "\n",
        "        subparser.set_defaults(func=train_model_from_args)\n",
        "\n",
        "        return subparser\n",
        "\n",
        "def train_model_from_args(args: argparse.Namespace):\n",
        "    \"\"\"\n",
        "    Just converts from an ``argparse.Namespace`` object to string paths.\n",
        "    \"\"\"\n",
        "    train_model_from_file(args.param_path,\n",
        "                          args.serialization_dir,\n",
        "                          args.overrides,\n",
        "                          args.file_friendly_logging,\n",
        "                          args.recover)\n",
        "\n",
        "\n",
        "def train_model_from_file(parameter_filename: str,\n",
        "                          serialization_dir: str,\n",
        "                          overrides: str = \"\",\n",
        "                          file_friendly_logging: bool = False,\n",
        "                          recover: bool = False) -> Model:\n",
        "    \"\"\"\n",
        "    A wrapper around :func:`train_model` which loads the params from a file.\n",
        "    Parameters\n",
        "    ----------\n",
        "    param_path : ``str``\n",
        "        A json parameter file specifying an AllenNLP experiment.\n",
        "    serialization_dir : ``str``\n",
        "        The directory in which to save results and logs. We just pass this along to\n",
        "        :func:`train_model`.\n",
        "    overrides : ``str``\n",
        "        A HOCON string that we will use to override values in the input parameter file.\n",
        "    file_friendly_logging : ``bool``, optional (default=False)\n",
        "        If ``True``, we make our output more friendly to saved model files.  We just pass this\n",
        "        along to :func:`train_model`.\n",
        "    recover : ``bool`, optional (default=False)\n",
        "        If ``True``, we will try to recover a training run from an existing serialization\n",
        "        directory.  This is only intended for use when something actually crashed during the middle\n",
        "        of a run.  For continuing training a model on new data, see the ``fine-tune`` command.\n",
        "    \"\"\"\n",
        "    # Load the experiment config from a file and pass it to ``train_model``.\n",
        "    params = Params.from_file(parameter_filename, overrides)\n",
        "    return train_model(params, serialization_dir, file_friendly_logging, recover)\n",
        "\n",
        "\n",
        "def datasets_from_params(params: Params) -> Dict[str, Iterable[Instance]]:\n",
        "    \"\"\"\n",
        "    Load all the datasets specified by the config.\n",
        "    \"\"\"\n",
        "    dataset_reader = DatasetReader.from_params(params.pop('dataset_reader'))\n",
        "    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n",
        "\n",
        "    validation_and_test_dataset_reader: DatasetReader = dataset_reader\n",
        "    if validation_dataset_reader_params is not None:\n",
        "        logger.info(\"Using a separate dataset reader to load validation and test data.\")\n",
        "        validation_and_test_dataset_reader = DatasetReader.from_params(validation_dataset_reader_params)\n",
        "\n",
        "    train_data_path = params.pop('train_data_path')\n",
        "    logger.info(\"Reading training data from %s\", train_data_path)\n",
        "    train_data = dataset_reader.read(train_data_path)\n",
        "\n",
        "    datasets: Dict[str, Iterable[Instance]] = {\"train\": train_data}\n",
        "\n",
        "    validation_data_path = params.pop('validation_data_path', None)\n",
        "    if validation_data_path is not None:\n",
        "        logger.info(\"Reading validation data from %s\", validation_data_path)\n",
        "        validation_data = validation_and_test_dataset_reader.read(validation_data_path)\n",
        "        datasets[\"validation\"] = validation_data\n",
        "\n",
        "    test_data_path = params.pop(\"test_data_path\", None)\n",
        "    if test_data_path is not None:\n",
        "        logger.info(\"Reading test data from %s\", test_data_path)\n",
        "        test_data = validation_and_test_dataset_reader.read(test_data_path)\n",
        "        datasets[\"test\"] = test_data\n",
        "\n",
        "    return datasets\n",
        "\n",
        "def create_serialization_dir(params: Params, serialization_dir: str, recover: bool) -> None:\n",
        "    \"\"\"\n",
        "    This function creates the serialization directory if it doesn't exist.  If it already exists,\n",
        "    then it verifies that we're recovering from a training with an identical configuration.\n",
        "    Parameters\n",
        "    ----------\n",
        "    params: ``Params``\n",
        "        A parameter object specifying an AllenNLP Experiment.\n",
        "    serialization_dir: ``str``\n",
        "        The directory in which to save results and logs.\n",
        "    recover: ``bool``\n",
        "        If ``True``, we will try to recover from an existing serialization directory, and crash if\n",
        "        the directory doesn't exist, or doesn't match the configuration we're given.\n",
        "    \"\"\"\n",
        "    if os.path.exists(serialization_dir):\n",
        "        if serialization_dir == '/output':\n",
        "            # Special-casing the beaker output directory, which will already exist when training\n",
        "            # starts.\n",
        "            return\n",
        "        if not recover:\n",
        "            raise ConfigurationError(f\"Serialization directory ({serialization_dir}) already exists.  \"\n",
        "                                     f\"Specify --recover to recover training from existing output.\")\n",
        "\n",
        "        logger.info(f\"Recovering from prior training at {serialization_dir}.\")\n",
        "\n",
        "        recovered_config_file = os.path.join(serialization_dir, CONFIG_NAME)\n",
        "        if not os.path.exists(recovered_config_file):\n",
        "            raise ConfigurationError(\"The serialization directory already exists but doesn't \"\n",
        "                                     \"contain a config.json. You probably gave the wrong directory.\")\n",
        "        else:\n",
        "            loaded_params = Params.from_file(recovered_config_file)\n",
        "\n",
        "            # Check whether any of the training configuration differs from the configuration we are\n",
        "            # resuming.  If so, warn the user that training may fail.\n",
        "            fail = False\n",
        "            flat_params = params.as_flat_dict()\n",
        "            flat_loaded = loaded_params.as_flat_dict()\n",
        "            for key in flat_params.keys() - flat_loaded.keys():\n",
        "                logger.error(f\"Key '{key}' found in training configuration but not in the serialization \"\n",
        "                             f\"directory we're recovering from.\")\n",
        "                fail = True\n",
        "            for key in flat_loaded.keys() - flat_params.keys():\n",
        "                logger.error(f\"Key '{key}' found in the serialization directory we're recovering from \"\n",
        "                             f\"but not in the training config.\")\n",
        "                fail = True\n",
        "            for key in flat_params.keys():\n",
        "                if flat_params.get(key, None) != flat_loaded.get(key, None):\n",
        "                    logger.error(f\"Value for '{key}' in training configuration does not match that the value in \"\n",
        "                                 f\"the serialization directory we're recovering from: \"\n",
        "                                 f\"{flat_params[key]} != {flat_loaded[key]}\")\n",
        "                    fail = True\n",
        "            if fail:\n",
        "                raise ConfigurationError(\"Training configuration does not match the configuration we're \"\n",
        "                                         \"recovering from.\")\n",
        "    else:\n",
        "        if recover:\n",
        "            raise ConfigurationError(f\"--recover specified but serialization_dir ({serialization_dir}) \"\n",
        "                                     \"does not exist.  There is nothing to recover from.\")\n",
        "        os.makedirs(serialization_dir)\n",
        "\n",
        "\n",
        "def train_model(params: Params,\n",
        "                serialization_dir: str,\n",
        "                file_friendly_logging: bool = False,\n",
        "                recover: bool = False) -> Model:\n",
        "    \"\"\"\n",
        "    Trains the model specified in the given :class:`Params` object, using the data and training\n",
        "    parameters also specified in that object, and saves the results in ``serialization_dir``.\n",
        "    Parameters\n",
        "    ----------\n",
        "    params : ``Params``\n",
        "        A parameter object specifying an AllenNLP Experiment.\n",
        "    serialization_dir : ``str``\n",
        "        The directory in which to save results and logs.\n",
        "    file_friendly_logging : ``bool``, optional (default=False)\n",
        "        If ``True``, we add newlines to tqdm output, even on an interactive terminal, and we slow\n",
        "        down tqdm's output to only once every 10 seconds.\n",
        "    recover : ``bool`, optional (default=False)\n",
        "        If ``True``, we will try to recover a training run from an existing serialization\n",
        "        directory.  This is only intended for use when something actually crashed during the middle\n",
        "        of a run.  For continuing training a model on new data, see the ``fine-tune`` command.\n",
        "    \"\"\"\n",
        "    prepare_environment(params)\n",
        "\n",
        "    create_serialization_dir(params, serialization_dir, recover)\n",
        "\n",
        "    # TODO(mattg): pull this block out into a separate function (maybe just add this to\n",
        "    # `prepare_environment`?)\n",
        "    Tqdm.set_slower_interval(file_friendly_logging)\n",
        "    sys.stdout = TeeLogger(os.path.join(serialization_dir, \"stdout.log\"), # type: ignore\n",
        "                           sys.stdout,\n",
        "                           file_friendly_logging)\n",
        "    sys.stderr = TeeLogger(os.path.join(serialization_dir, \"stderr.log\"), # type: ignore\n",
        "                           sys.stderr,\n",
        "                           file_friendly_logging)\n",
        "    handler = logging.FileHandler(os.path.join(serialization_dir, \"python_logging.log\"))\n",
        "    handler.setLevel(logging.INFO)\n",
        "    handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s'))\n",
        "    logging.getLogger().addHandler(handler)\n",
        "\n",
        "    serialization_params = deepcopy(params).as_dict(quiet=True)\n",
        "    with open(os.path.join(serialization_dir, CONFIG_NAME), \"w\") as param_file:\n",
        "        json.dump(serialization_params, param_file, indent=4)\n",
        "\n",
        "    all_datasets = datasets_from_params(params)\n",
        "    datasets_for_vocab_creation = set(params.pop(\"datasets_for_vocab_creation\", all_datasets))\n",
        "\n",
        "    for dataset in datasets_for_vocab_creation:\n",
        "        if dataset not in all_datasets:\n",
        "            raise ConfigurationError(f\"invalid 'dataset_for_vocab_creation' {dataset}\")\n",
        "\n",
        "    logger.info(\"Creating a vocabulary using %s data.\", \", \".join(datasets_for_vocab_creation))\n",
        "    vocab = Vocabulary.from_params(params.pop(\"vocabulary\", {}),\n",
        "                                   (instance for key, dataset in all_datasets.items()\n",
        "                                    for instance in dataset\n",
        "                                    if key in datasets_for_vocab_creation))\n",
        "    vocab.save_to_files(os.path.join(serialization_dir, \"vocabulary\"))\n",
        "\n",
        "    model = Model.from_params(vocab, params.pop('model'))\n",
        "    iterator = DataIterator.from_params(params.pop(\"iterator\"))\n",
        "    iterator.index_with(vocab)\n",
        "\n",
        "    train_data = all_datasets['train']\n",
        "    validation_data = all_datasets.get('validation')\n",
        "    test_data = all_datasets.get('test')\n",
        "\n",
        "    trainer_params = params.pop(\"trainer\")\n",
        "    trainer = Trainer.from_params(model,\n",
        "                                  serialization_dir,\n",
        "                                  iterator,\n",
        "                                  train_data,\n",
        "                                  validation_data,\n",
        "                                  trainer_params)\n",
        "\n",
        "    evaluate_on_test = params.pop_bool(\"evaluate_on_test\", False)\n",
        "    params.assert_empty('base train command')\n",
        "    metrics = trainer.train()\n",
        "\n",
        "    # Now tar up results\n",
        "    archive_model(serialization_dir, files_to_archive=params.files_to_archive)\n",
        "\n",
        "    if test_data and evaluate_on_test:\n",
        "        test_metrics = evaluate(model, test_data, iterator, cuda_device=trainer._cuda_devices[0])  # pylint: disable=protected-access\n",
        "        for key, value in test_metrics.items():\n",
        "            metrics[\"test_\" + key] = value\n",
        "\n",
        "    elif test_data:\n",
        "        logger.info(\"To evaluate on the test set after training, pass the \"\n",
        "                    \"'evaluate_on_test' flag, or use the 'allennlp evaluate' command.\")\n",
        "\n",
        "    metrics_json = json.dumps(metrics, indent=2)\n",
        "    with open(os.path.join(serialization_dir, \"metrics.json\"), \"w\") as metrics_file:\n",
        "        metrics_file.write(metrics_json)\n",
        "    logger.info(\"Metrics: %s\", metrics_json)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "vE0yJovYm7r3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Run the model"
      ]
    },
    {
      "metadata": {
        "id": "tuuA0Gd6xHCE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "experiment_parameters = 'https://raw.githubusercontent.com/dmesquita/easy-deep-learning-with-AllenNLP/master/experiments/newsgroups_with_cuda.json'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fol-0Q48wRDQ",
        "colab_type": "code",
        "outputId": "62c3fe64-59b4-4fcf-e022-39dab5daab60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        }
      },
      "cell_type": "code",
      "source": [
        "train_model_from_file(experiment_parameters,\"/temp_dir\") "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1774it [00:16, 105.37it/s]\n",
            "1180it [00:12, 96.48it/s]\n",
            "2954it [00:02, 1404.15it/s]\n",
            "accuracy: 0.33, accuracy3: 0.96, loss: 1.45 ||: 100%|##########| 28/28 [00:06<00:00,  4.07it/s]\n",
            "accuracy: 0.37, accuracy3: 1.00, loss: 1.12 ||: 100%|##########| 28/28 [00:04<00:00,  5.78it/s]\n",
            "accuracy: 0.44, accuracy3: 1.00, loss: 1.05 ||: 100%|##########| 28/28 [00:04<00:00,  5.70it/s]\n",
            "accuracy: 0.54, accuracy3: 1.00, loss: 0.97 ||: 100%|##########| 28/28 [00:04<00:00,  5.76it/s]\n",
            "accuracy: 0.59, accuracy3: 1.00, loss: 0.88 ||: 100%|##########| 28/28 [00:04<00:00,  5.75it/s]\n",
            "accuracy: 0.65, accuracy3: 1.00, loss: 0.77 ||: 100%|##########| 28/28 [00:04<00:00,  5.76it/s]\n",
            "accuracy: 0.68, accuracy3: 1.00, loss: 0.74 ||: 100%|##########| 28/28 [00:04<00:00,  5.71it/s]\n",
            "accuracy: 0.75, accuracy3: 1.00, loss: 0.61 ||: 100%|##########| 28/28 [00:04<00:00,  5.72it/s]\n",
            "accuracy: 0.78, accuracy3: 1.00, loss: 0.55 ||: 100%|##########| 28/28 [00:04<00:00,  5.76it/s]\n",
            "accuracy: 0.80, accuracy3: 1.00, loss: 0.48 ||:  21%|##1       | 6/28 [00:00<00:03,  6.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.77, accuracy3: 1.00, loss: 0.55 ||: 100%|##########| 28/28 [00:04<00:00,  5.71it/s]\n",
            "accuracy: 0.81, accuracy3: 1.00, loss: 0.48 ||: 100%|##########| 28/28 [00:04<00:00,  5.73it/s]\n",
            "accuracy: 0.85, accuracy3: 1.00, loss: 0.40 ||: 100%|##########| 28/28 [00:04<00:00,  5.78it/s]\n",
            "accuracy: 0.89, accuracy3: 1.00, loss: 0.30 ||: 100%|##########| 28/28 [00:04<00:00,  5.72it/s]\n",
            "accuracy: 0.90, accuracy3: 1.00, loss: 0.28 ||: 100%|##########| 28/28 [00:04<00:00,  5.78it/s]\n",
            "accuracy: 0.91, accuracy3: 1.00, loss: 0.25 ||: 100%|##########| 28/28 [00:04<00:00,  5.75it/s]\n",
            "accuracy: 0.90, accuracy3: 1.00, loss: 0.27 ||: 100%|##########| 28/28 [00:04<00:00,  5.79it/s]\n",
            "accuracy: 0.94, accuracy3: 1.00, loss: 0.16 ||: 100%|##########| 28/28 [00:04<00:00,  5.76it/s]\n",
            "accuracy: 0.96, accuracy3: 1.00, loss: 0.13 ||: 100%|##########| 28/28 [00:04<00:00,  5.73it/s]\n",
            "accuracy: 0.97, accuracy3: 1.00, loss: 0.09 ||: 100%|##########| 28/28 [00:05<00:00,  5.47it/s]\n",
            "accuracy: 0.97, accuracy3: 1.00, loss: 0.10 ||:  43%|####2     | 12/28 [00:03<00:04,  3.55it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.97, accuracy3: 1.00, loss: 0.09 ||: 100%|##########| 28/28 [00:04<00:00,  5.74it/s]\n",
            "accuracy: 0.98, accuracy3: 1.00, loss: 0.07 ||: 100%|##########| 28/28 [00:04<00:00,  5.77it/s]\n",
            "accuracy: 0.99, accuracy3: 1.00, loss: 0.04 ||: 100%|##########| 28/28 [00:04<00:00,  5.72it/s]\n",
            "accuracy: 0.99, accuracy3: 1.00, loss: 0.04 ||: 100%|##########| 28/28 [00:04<00:00,  5.78it/s]\n",
            "accuracy: 0.99, accuracy3: 1.00, loss: 0.03 ||: 100%|##########| 28/28 [00:04<00:00,  5.73it/s]\n",
            "accuracy: 1.00, accuracy3: 1.00, loss: 0.02 ||: 100%|##########| 28/28 [00:04<00:00,  5.78it/s]\n",
            "accuracy: 1.00, accuracy3: 1.00, loss: 0.01 ||: 100%|##########| 28/28 [00:04<00:00,  5.77it/s]\n",
            "accuracy: 1.00, accuracy3: 1.00, loss: 0.01 ||: 100%|##########| 28/28 [00:04<00:00,  5.79it/s]\n",
            "accuracy: 1.00, accuracy3: 1.00, loss: 0.02 ||: 100%|##########| 28/28 [00:04<00:00,  5.73it/s]\n",
            "accuracy: 1.00, accuracy3: 1.00, loss: 0.01 ||: 100%|##########| 28/28 [00:04<00:00,  5.69it/s]\n",
            "accuracy: 1.00, accuracy3: 1.00, loss: 0.01 ||:  64%|######4   | 18/28 [00:02<00:01,  8.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 1.00, accuracy3: 1.00, loss: 0.01 ||: 100%|##########| 28/28 [00:04<00:00,  5.72it/s]\n",
            "accuracy: 0.84, accuracy3: 1.00 ||: 100%|##########| 19/19 [00:03<00:00,  5.91it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Fetch20NewsgroupsClassifier(\n",
              "  (model_text_field_embedder): BasicTextFieldEmbedder(\n",
              "    (token_embedder_tokens): Embedding(\n",
              "    )\n",
              "  )\n",
              "  (internal_text_encoder): PytorchSeq2VecWrapper(\n",
              "    (_module): LSTM(100, 100, batch_first=True, dropout=0.2, bidirectional=True)\n",
              "  )\n",
              "  (classifier_feedforward): FeedForward(\n",
              "    (_linear_layers): ModuleList(\n",
              "      (0): Linear(in_features=200, out_features=200)\n",
              "      (1): Linear(in_features=200, out_features=100)\n",
              "    )\n",
              "    (_dropout): ModuleList(\n",
              "      (0): Dropout(p=0.2)\n",
              "      (1): Dropout(p=0.0)\n",
              "    )\n",
              "  )\n",
              "  (loss): CrossEntropyLoss(\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "0x3GwEqQclEE",
        "colab_type": "code",
        "outputId": "d642045d-7075-4a97-b9fc-5e590c0d2fae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "zx11GjtT3IRI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}